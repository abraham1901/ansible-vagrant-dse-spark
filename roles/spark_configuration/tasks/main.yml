---
# file: roles/spark/tasks/main.yml

- name: create spark directory
  file: path={{ item }} state=directory
  with_items: spark.location

- name: create log direcory
  file: path={{spark.eventLog}} state=directory mode=777
  tags:
      - spark
      - start-spark-history

- name: cleanup tmp for spark distro
  shell: rm -rf /tmp/spark*

- name: cleanup target dir for spark
  shell: rm -rf {{ spark.location }}

- name: create target dir for spark
  shell: mkdir -p {{ spark.location }}

- name: upload spark disto from local host, don\'t forget to run download spark locally once before provisioning
  copy: src=spark-distro/spark-1.2.0-bin-hadoop2.4.tgz dest=/tmp owner=vagrant group=vagrant mode=0777

- name: extract spark from uploaded acrhive
  shell: chdir=/tmp tar zxvf /tmp/spark-1.2.0-bin-hadoop2.4.tgz

- name: copy extracted spark binaries to target
  shell: cp -rf /tmp/spark-1.2.0-bin-hadoop2.4/* {{ spark.location }}

- name: copy spark-env.sh
  template: src=spark-env.sh dest={{ spark.location }}/conf/spark-env.sh

- name: create conf for spark
  file: path=/etc/spark/conf state=directory

- name: copy spark-defaults.conf to /etc/spark/conf
  template: src=spark-defaults.conf dest=/etc/spark/conf/spark-defaults.conf

- name: copy spark-defaults.conf to spark.location
  template: src=spark-defaults.conf dest={{ spark.location }}/conf/spark-defaults.conf
